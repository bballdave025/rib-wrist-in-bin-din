{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNHXOOjZrrCxVTE/IDf9SJH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bballdave025/rib-wrist-in-bin-din/blob/main/Paper_Code_Prep_01_-_no_output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Model Structures for RMFB Paper<br/>Part I: CNN Baseline and No-Frills Resnet50"
      ],
      "metadata": {
        "id": "v7HLCE5MhepV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rebuilding All Models with Latest Versions as well as<br/>Doing Learned Visualization of Architecture Structure<sup>\\[NB1\\]\\[NB2\\]\\[NB3\\]</sup><br/>and Especially Grad-CAM<sup>\\[NB4\\]</sup> Visual Classification Explanations  "
      ],
      "metadata": {
        "id": "yoS2pEW-yXmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "David BLACK, GitHub @bballdave025 ,\n",
        "\n",
        "Working Title for Technical Paper: &quot;AI Computer Vision Methods for\n",
        "Finding Information-bearing Writing Surfaces (Especially Codex Fragments) Reused in Other Codex Bindings&quot;\n",
        "\n",
        "Working Title for <em>Fragmentology</em><sup>[NB5]</sup>\n",
        "(Manuscript Studies) Paper: None Yet\n",
        "(It would be nice to have some statistics.)"
      ],
      "metadata": {
        "id": "wA_FVw676E1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Why these Notebooks?\n",
        "\n",
        "There are several reasons for writing these notebooks and doing these various visualizations. The most acceptable (I almost put that in quotation marks; I mean the most-what-employers-or-theoretical-AI/ML-people-would-like-to-hear) answers are: <br/>&nbsp;&nbsp;&nbsp;1) I want to give <strong>greater explainability of the CV model  decisions</strong>,  especially as it relates to the audience of the paper and their acceptance of a non-manual, non-human method, possibly seen as an intruder  into the Manuscript Studies community. I think that if they see the model  making decisions in the same way that <em>they</em> would make decisions, they will be more likely to see it as an aide doing such things as allowing grad students to analyze fragments rather than finding them. For clarity, I mention that the audience consists of the readers of the Binding-Fragment paper to be submitted to <em>Fragmentology</em>; <br/>&nbsp;&nbsp;&nbsp;2) I want to make better decisions as to which model will most-likely make the <strong>goal of ~95% precision and ~70% recall possible on a wide variety of writing-material and binding images</strong>. The ultimate goal for (this stage of) the study is to run the algorithm over as many of FamilySearch's earliest-to-18th Century images (let's make it readable: images from the earliest records they have up to the 1700s) as possible. Through the analysis and visualization of how well the chosen model or models performs, we can have higher confidence that an eventual <strong>one run</strong> over what I'll estimate as 0.5 to 1 billion images with a thoroughly tested and understood/explainable model will <strong>suffice for at least a decade</strong>. <br/>&nbsp;&nbsp;&nbsp;3) (This is a long one.) Especially with the Grad-CAM visualizations of the salient features for model decisions, I want to have something to <strong>show to the Manuscript Studies/Fragmentology community that will get them excited</strong> about <em>helping us</em> to <em>create better training sets</em> and to <em>help create new training sets for interesting phenomena/occurences</em> (for lack of a better word describing what each of the classifications describes). I think that the best example of this is to create a model (or add to the existing model) the search for images that not only have iron-gall-ink damage to the point of going through the page (something I have tried to do without expertise), but which are at different stages<sup>\\[NB5\\]</sup> of the iron-gall-ink (and other corrosive dye) damage process. I'm also going to face the fact that, even as concerns the main object of study—fragments found in bindings—I'm perhaps passingly knowledgeable but by no means an expert. Seeing heat maps describing what the model thinks is most important in finding a specific images I've classified should make this a more interesting invitation, as it allows quick view of the types of phenomena such models can find in and on writing materials, bindings, etc.\n",
        "\n",
        "For my estimate of the sum total of such documents, I make reference to the <em>Company Facts</em> page on FamilySearch [as I see it now (archived with numbers updated 2025-04-07)](https://web.archive.org/web/20250423044740/https://www.familysearch.org/en/newsroom/company-facts), which reports \"5.65 Billion \\[Digital Images Published\\]\" . For a second opinion, I've looked at the running counter of images digitized at https://www.familysearch.org/en/records/images/, which is at \\[let me go take a screenshot\\]\n",
        "\n",
        "<br/>\n",
        "<div>\n",
        "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/FamilySearch_5667347462_5point667-B_Images_2025-04-22T113500-0600.png\"\n",
        "       alt=\"Screenshot of family search dot org forward slash E N forward slash records forward slash images. Tab name is. QUOTE. Explore Historical Images. END QUOTE. Visible text is. QUOTE. Access billions of documents newline FamilySearch has been collecting historical documents since eighteen ninety four and currently has. Left square bracket. Transcriber's note colon. Number in short scale, i.e. without milliard, billiard, et cetera, so ten to the ninth is read billion, ten to the twelfth is read trillion, et cetera. Right bracket. five billion six hundred sixty-seven million, three hundred forty-seven thousand four hundred sixty-one images from all over the world. That makes FamilySearch one of the world's largest collections of historical documents exclamation point. END QUOTE. Bottom toolbar shows eleven colon thirty-five A M. and. four slash twenty-two forward slash two thousand twenty-five. Left Bracket. Date format is as in the stupid U S A format comma month slash day slash year. Right Bracket. An errow points from the annotated text. QUOTE. M D T. Left parenthesis. G M T minus six Right parenthesis. END QUOTE. to the time and date.\"\n",
        "       width=\"500px\">\n",
        "</div>\n",
        "<br/>\n",
        "\n",
        "5,667,347,461 (five billion six hundred sixty-seven million, three hundred forty-seven thousand four hundred sixty-one) images. The type of binding we're seeking is most-commonly found in Western and Central Europe going into Russia and Persia, with some around the Mediterranean—including Muslim and Jewish records—the Near East, and the Americas—mostly (Catholic) South America. Since I think that records from the current United States of America probably take up at least a third of all digital images published, and Western/Central Europe <em>at least</em> one quarter of the remaining digital images, my guess is that a search in those geographical regions from the earliest date for which FamilySearch has records to 1799 will yield approximately<br/>$ \\frac{2}{3} \\cdot \\frac{1}{4} \\cdot 5.66 \\times 10^{9} $ images or $ 9.45 \\times 10^{8} \\left(\\substack{+9e8 \\\\ -5e9} \\: \\mathrm{systematic}\\right) \\left(\\pm 0.25e9 \\;  \\mathrm{who} \\! \\cdot \\! \\mathrm{knows}\\right) $,<br/>$ \\frac{1}{6}\\;\\mathrm{ish} $ of the records, 500 million to 1.5 billion (again, very $ \\mathrm{ish} $).<br/>(Note that my methodology will let in records from the 1800s and even a few from the 1900s, but not on purpose.)\n",
        "\n",
        "The current study will include around hundreds of thousands of images, with an approximate guess of half being from FamilySearch, for the intial study and publication in <em>Fragmentology</em>. This study entails at least thousands of images, up to tens of thousands, for the training, validation, and test sets. (That is, tens of thousands of images split into the three sets.)"
      ],
      "metadata": {
        "id": "hKsG7S_F6ZP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#no-more#!date +'%s_%Y-%m-%dT%H:%M:%S%z'"
      ],
      "metadata": {
        "id": "cbxn1-lL5N_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Original output when first writing the re-do\n",
        "\n",
        "`1745382382_2025-04-23T04:26:22+0000`\n",
        "\n",
        "Local: `2025-04-22T22:26:22-0600`"
      ],
      "metadata": {
        "id": "F45IYsaN5a3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports, noting deprecated imports that would have been used in previous versions"
      ],
      "metadata": {
        "id": "OhbCzfP7hp_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import models\n",
        "  #  for: Sequential\n",
        "# from [tensorflow.]keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "  #  for: Dense, Activation, Conv2D, MaxPooling2D,\n",
        "  #+      Dropout, Flatten, Input\n",
        "\n",
        "#  from [tensorflow.].keras.layers import \\\n",
        "#+               Dense, Dropout, Activation,\n",
        "#+               Flatten, Conv2D, MaxPooling2D\n",
        "# from [tensorflow.]keras.layers import Activation\n",
        "# from [tensorflow.]keras.layers.core import Dense, Flatten\n",
        "# from [tensorflow.]keras.layers.core import Dropout\n",
        "# from [tensorflow.]keras.layers.convolutional import Conv2D\n",
        "# from [tensorflow.]keras.layers.pooling import MaxPooling2D\n",
        "\n",
        "from tensorflow.keras import optimizers\n",
        "  # for: Adam\n",
        "#from [tensorflow.]keras.optimizers import Adam\n",
        "from tensorflow.keras import metrics\n",
        "  # for: categorical_crossentropy\n",
        "#from [tensorflow.]keras.metrics import categorical_crossentropy\n",
        "\n",
        "from tensorflow.keras import losses\n",
        "  # for: sparse_categorical_crossentropy\n",
        "#from [tensorflow.]keras.losses import sparse_categorical_crossentropy\n",
        "\n",
        "# Callbacks (logging and seeing the model train)\n",
        "from tensorflow.keras import callbacks\n",
        "  #  for: CSVLogger, Callback  ## custom callback\n",
        "  #+      (maybe EarlyStopping, ModelCheckpoint)\n",
        "#from [tensorflow.]keras.callbacks import CSVLogger\n",
        "\n",
        "import tensorboard\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "##%matplotlib inline  # Oh yeah, Colab doesn't like this.\n",
        "import numpy as np\n",
        "\n",
        "##----------------\n",
        "#  VISUALIZATIONS\n",
        "##----------------\n",
        "from PIL import ImageFont"
      ],
      "metadata": {
        "id": "8B4BBRaIshZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Packages requiring installation on Google Colab"
      ],
      "metadata": {
        "id": "R9-KaVcn1DN2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  For nice display of durations\n",
        "!pip install --quiet humanfriendly\n",
        "from humanfriendly import format_timespan\n",
        "\n",
        "\n",
        "#  For visualizations of architectures\n",
        "!pip install --quiet visualkeras\n",
        "import visualkeras\n",
        "\n",
        "!pip install --quiet keras_sequential_ascii\n",
        "from keras_sequential_ascii import keras2ascii"
      ],
      "metadata": {
        "id": "uVBbiz_i02yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Other Imports for More Specialized Parts\n",
        "(e.g. looking at documentation, timing things, ...)"
      ],
      "metadata": {
        "id": "P6pAzGqR4u1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect\n",
        "import timeit\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "5lK5TlDHgZnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Specifics for Colab Environment - I would minimize it unless you need it"
      ],
      "metadata": {
        "id": "8WtLQZJE96mL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this <strike>commit</strike> pull request."
      ],
      "metadata": {
        "id": "KqICUMmb-SUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!date +'%s_%Y-%m-%dT%H:%M:%S%z'"
      ],
      "metadata": {
        "id": "eI4CfCqb97So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output with this commit:\n",
        "\n",
        "`1745451271_2025-04-23T23:34:31+0000`\n",
        "\n",
        "Local: `2025-04-23T17:34:31-0600`\n",
        "\n",
        "(I'll try to keep this updated for each commit, or at least for each pull request.)"
      ],
      "metadata": {
        "id": "4_5zRBxp-h0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Uncomment and run before merging (or after merging) each pull request."
      ],
      "metadata": {
        "id": "y5QslqzV-0hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze"
      ],
      "metadata": {
        "id": "7qsCaHgm-kP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output of `pip freeze` at approximately `1745451271_2025-04-23T23:34:31+0000`\n",
        "\n",
        "```\n",
        "absl-py==1.4.0\n",
        "accelerate==1.5.2\n",
        "aggdraw==1.3.19\n",
        "aiohappyeyeballs==2.6.1\n",
        "aiohttp==3.11.15\n",
        "aiosignal==1.3.2\n",
        "alabaster==1.0.0\n",
        "albucore==0.0.23\n",
        "albumentations==2.0.5\n",
        "ale-py==0.10.2\n",
        "altair==5.5.0\n",
        "annotated-types==0.7.0\n",
        "anyio==4.9.0\n",
        "argon2-cffi==23.1.0\n",
        "argon2-cffi-bindings==21.2.0\n",
        "array_record==0.7.1\n",
        "arviz==0.21.0\n",
        "astropy==7.0.1\n",
        "astropy-iers-data==0.2025.4.14.0.37.22\n",
        "astunparse==1.6.3\n",
        "atpublic==5.1\n",
        "attrs==25.3.0\n",
        "audioread==3.0.1\n",
        "autograd==1.7.0\n",
        "babel==2.17.0\n",
        "backcall==0.2.0\n",
        "backports.tarfile==1.2.0\n",
        "beautifulsoup4==4.13.4\n",
        "betterproto==2.0.0b6\n",
        "bigframes==1.42.0\n",
        "bigquery-magics==0.9.0\n",
        "bleach==6.2.0\n",
        "blinker==1.9.0\n",
        "blis==1.3.0\n",
        "blosc2==3.3.0\n",
        "bokeh==3.6.3\n",
        "Bottleneck==1.4.2\n",
        "bqplot==0.12.44\n",
        "branca==0.8.1\n",
        "CacheControl==0.14.2\n",
        "cachetools==5.5.2\n",
        "catalogue==2.0.10\n",
        "certifi==2025.1.31\n",
        "cffi==1.17.1\n",
        "chardet==5.2.0\n",
        "charset-normalizer==3.4.1\n",
        "chex==0.1.89\n",
        "clarabel==0.10.0\n",
        "click==8.1.8\n",
        "cloudpathlib==0.21.0\n",
        "cloudpickle==3.1.1\n",
        "cmake==3.31.6\n",
        "cmdstanpy==1.2.5\n",
        "colorcet==3.1.0\n",
        "colorlover==0.3.0\n",
        "colour==0.1.5\n",
        "community==1.0.0b1\n",
        "confection==0.1.5\n",
        "cons==0.4.6\n",
        "contourpy==1.3.2\n",
        "cramjam==2.10.0\n",
        "cryptography==43.0.3\n",
        "cuda-python==12.6.2.post1\n",
        "cudf-cu12 @ https://pypi.nvidia.com/cudf-cu12/cudf_cu12-25.2.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
        "cudf-polars-cu12==25.2.2\n",
        "cufflinks==0.17.3\n",
        "cuml-cu12==25.2.1\n",
        "cupy-cuda12x==13.3.0\n",
        "cuvs-cu12==25.2.1\n",
        "cvxopt==1.3.2\n",
        "cvxpy==1.6.5\n",
        "cycler==0.12.1\n",
        "cyipopt==1.5.0\n",
        "cymem==2.0.11\n",
        "Cython==3.0.12\n",
        "dask==2024.12.1\n",
        "dask-cuda==25.2.0\n",
        "dask-cudf-cu12==25.2.2\n",
        "dask-expr==1.1.21\n",
        "datascience==0.17.6\n",
        "db-dtypes==1.4.2\n",
        "dbus-python==1.2.18\n",
        "debugpy==1.8.0\n",
        "decorator==4.4.2\n",
        "defusedxml==0.7.1\n",
        "Deprecated==1.2.18\n",
        "diffusers==0.32.2\n",
        "distributed==2024.12.1\n",
        "distributed-ucxx-cu12==0.42.0\n",
        "distro==1.9.0\n",
        "dlib==19.24.6\n",
        "dm-tree==0.1.9\n",
        "docker-pycreds==0.4.0\n",
        "docstring_parser==0.16\n",
        "docutils==0.21.2\n",
        "dopamine_rl==4.1.2\n",
        "duckdb==1.2.2\n",
        "earthengine-api==1.5.11\n",
        "easydict==1.13\n",
        "editdistance==0.8.1\n",
        "eerepr==0.1.1\n",
        "einops==0.8.1\n",
        "en_core_web_sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl#sha256=1932429db727d4bff3deed6b34cfc05df17794f4a52eeb26cf8928f7c1a0fb85\n",
        "entrypoints==0.4\n",
        "et_xmlfile==2.0.0\n",
        "etils==1.12.2\n",
        "etuples==0.3.9\n",
        "Farama-Notifications==0.0.4\n",
        "fastai==2.7.19\n",
        "fastcore==1.7.29\n",
        "fastdownload==0.0.7\n",
        "fastjsonschema==2.21.1\n",
        "fastprogress==1.0.3\n",
        "fastrlock==0.8.3\n",
        "filelock==3.18.0\n",
        "firebase-admin==6.7.0\n",
        "Flask==3.1.0\n",
        "flatbuffers==25.2.10\n",
        "flax==0.10.5\n",
        "folium==0.19.5\n",
        "fonttools==4.57.0\n",
        "frozendict==2.4.6\n",
        "frozenlist==1.5.0\n",
        "fsspec==2025.3.2\n",
        "future==1.0.0\n",
        "gast==0.6.0\n",
        "gcsfs==2025.3.2\n",
        "GDAL==3.6.4\n",
        "gdown==5.2.0\n",
        "geemap==0.35.3\n",
        "geocoder==1.38.1\n",
        "geographiclib==2.0\n",
        "geopandas==1.0.1\n",
        "geopy==2.4.1\n",
        "gin-config==0.5.0\n",
        "gitdb==4.0.12\n",
        "GitPython==3.1.44\n",
        "glob2==0.7\n",
        "google==2.0.3\n",
        "google-ai-generativelanguage==0.6.15\n",
        "google-api-core==2.24.2\n",
        "google-api-python-client==2.164.0\n",
        "google-auth==2.38.0\n",
        "google-auth-httplib2==0.2.0\n",
        "google-auth-oauthlib==1.2.1\n",
        "google-cloud-aiplatform==1.88.0\n",
        "google-cloud-bigquery==3.31.0\n",
        "google-cloud-bigquery-connection==1.18.2\n",
        "google-cloud-bigquery-storage==2.30.0\n",
        "google-cloud-bigtable==2.30.0\n",
        "google-cloud-core==2.4.3\n",
        "google-cloud-dataproc==5.18.1\n",
        "google-cloud-datastore==2.21.0\n",
        "google-cloud-firestore==2.20.1\n",
        "google-cloud-functions==1.20.3\n",
        "google-cloud-iam==2.19.0\n",
        "google-cloud-language==2.17.1\n",
        "google-cloud-pubsub==2.29.0\n",
        "google-cloud-resource-manager==1.14.2\n",
        "google-cloud-spanner==3.53.0\n",
        "google-cloud-storage==2.19.0\n",
        "google-cloud-translate==3.20.2\n",
        "google-colab @ file:///colabtools/dist/google_colab-1.0.0.tar.gz\n",
        "google-crc32c==1.7.1\n",
        "google-genai==1.10.0\n",
        "google-generativeai==0.8.4\n",
        "google-pasta==0.2.0\n",
        "google-resumable-media==2.7.2\n",
        "google-spark-connect==0.5.2\n",
        "googleapis-common-protos==1.70.0\n",
        "googledrivedownloader==1.1.0\n",
        "graphviz==0.20.3\n",
        "greenlet==3.2.0\n",
        "grpc-google-iam-v1==0.14.2\n",
        "grpc-interceptor==0.15.4\n",
        "grpcio==1.71.0\n",
        "grpcio-status==1.71.0\n",
        "grpclib==0.4.7\n",
        "gspread==6.2.0\n",
        "gspread-dataframe==4.0.0\n",
        "gym==0.25.2\n",
        "gym-notices==0.0.8\n",
        "gymnasium==1.1.1\n",
        "h11==0.14.0\n",
        "h2==4.2.0\n",
        "h5netcdf==1.6.1\n",
        "h5py==3.13.0\n",
        "hdbscan==0.8.40\n",
        "highspy==1.9.0\n",
        "holidays==0.70\n",
        "holoviews==1.20.2\n",
        "hpack==4.1.0\n",
        "html5lib==1.1\n",
        "httpcore==1.0.8\n",
        "httpimport==1.4.1\n",
        "httplib2==0.22.0\n",
        "httpx==0.28.1\n",
        "huggingface-hub==0.30.2\n",
        "humanfriendly==10.0\n",
        "humanize==4.12.2\n",
        "hyperframe==6.1.0\n",
        "hyperopt==0.2.7\n",
        "ibis-framework==9.5.0\n",
        "idna==3.10\n",
        "imageio==2.37.0\n",
        "imageio-ffmpeg==0.6.0\n",
        "imagesize==1.4.1\n",
        "imbalanced-learn==0.13.0\n",
        "immutabledict==4.2.1\n",
        "importlib_metadata==8.6.1\n",
        "importlib_resources==6.5.2\n",
        "imutils==0.5.4\n",
        "inflect==7.5.0\n",
        "iniconfig==2.1.0\n",
        "intel-cmplr-lib-ur==2025.1.0\n",
        "intel-openmp==2025.1.0\n",
        "ipyevents==2.0.2\n",
        "ipyfilechooser==0.6.0\n",
        "ipykernel==6.17.1\n",
        "ipyleaflet==0.19.2\n",
        "ipyparallel==8.8.0\n",
        "ipython==7.34.0\n",
        "ipython-genutils==0.2.0\n",
        "ipython-sql==0.5.0\n",
        "ipytree==0.2.2\n",
        "ipywidgets==7.7.1\n",
        "itsdangerous==2.2.0\n",
        "jaraco.classes==3.4.0\n",
        "jaraco.context==6.0.1\n",
        "jaraco.functools==4.1.0\n",
        "jax==0.5.2\n",
        "jax-cuda12-pjrt==0.5.1\n",
        "jax-cuda12-plugin==0.5.1\n",
        "jaxlib==0.5.1\n",
        "jeepney==0.9.0\n",
        "jellyfish==1.1.0\n",
        "jieba==0.42.1\n",
        "Jinja2==3.1.6\n",
        "jiter==0.9.0\n",
        "joblib==1.4.2\n",
        "jsonpatch==1.33\n",
        "jsonpickle==4.0.5\n",
        "jsonpointer==3.0.0\n",
        "jsonschema==4.23.0\n",
        "jsonschema-specifications==2024.10.1\n",
        "jupyter-client==6.1.12\n",
        "jupyter-console==6.1.0\n",
        "jupyter-leaflet==0.19.2\n",
        "jupyter-server==1.16.0\n",
        "jupyter_core==5.7.2\n",
        "jupyterlab_pygments==0.3.0\n",
        "jupyterlab_widgets==3.0.14\n",
        "kaggle==1.7.4.2\n",
        "kagglehub==0.3.11\n",
        "keras==3.8.0\n",
        "keras-hub==0.18.1\n",
        "keras-nlp==0.18.1\n",
        "keras_sequential_ascii==0.1.1\n",
        "keyring==25.6.0\n",
        "keyrings.google-artifactregistry-auth==1.1.2\n",
        "kiwisolver==1.4.8\n",
        "langchain==0.3.23\n",
        "langchain-core==0.3.52\n",
        "langchain-text-splitters==0.3.8\n",
        "langcodes==3.5.0\n",
        "langsmith==0.3.31\n",
        "language_data==1.3.0\n",
        "launchpadlib==1.10.16\n",
        "lazr.restfulclient==0.14.4\n",
        "lazr.uri==1.0.6\n",
        "lazy_loader==0.4\n",
        "libclang==18.1.1\n",
        "libcudf-cu12 @ https://pypi.nvidia.com/libcudf-cu12/libcudf_cu12-25.2.1-py3-none-manylinux_2_28_x86_64.whl\n",
        "libcugraph-cu12==25.2.0\n",
        "libcuml-cu12==25.2.1\n",
        "libcuvs-cu12==25.2.1\n",
        "libkvikio-cu12==25.2.1\n",
        "libraft-cu12==25.2.0\n",
        "librosa==0.11.0\n",
        "libucx-cu12==1.18.0\n",
        "libucxx-cu12==0.42.0\n",
        "lightgbm==4.5.0\n",
        "linkify-it-py==2.0.3\n",
        "llvmlite==0.43.0\n",
        "locket==1.0.0\n",
        "logical-unification==0.4.6\n",
        "lxml==5.3.2\n",
        "Mako==1.1.3\n",
        "marisa-trie==1.2.1\n",
        "Markdown==3.8\n",
        "markdown-it-py==3.0.0\n",
        "MarkupSafe==3.0.2\n",
        "matplotlib==3.10.0\n",
        "matplotlib-inline==0.1.7\n",
        "matplotlib-venn==1.1.2\n",
        "mdit-py-plugins==0.4.2\n",
        "mdurl==0.1.2\n",
        "miniKanren==1.0.3\n",
        "missingno==0.5.2\n",
        "mistune==3.1.3\n",
        "mizani==0.13.3\n",
        "mkl==2025.0.1\n",
        "ml-dtypes==0.4.1\n",
        "mlxtend==0.23.4\n",
        "more-itertools==10.6.0\n",
        "moviepy==1.0.3\n",
        "mpmath==1.3.0\n",
        "msgpack==1.1.0\n",
        "multidict==6.4.3\n",
        "multipledispatch==1.0.0\n",
        "multitasking==0.0.11\n",
        "murmurhash==1.0.12\n",
        "music21==9.3.0\n",
        "namex==0.0.8\n",
        "narwhals==1.35.0\n",
        "natsort==8.4.0\n",
        "nbclassic==1.2.0\n",
        "nbclient==0.10.2\n",
        "nbconvert==7.16.6\n",
        "nbformat==5.10.4\n",
        "ndindex==1.9.2\n",
        "nest-asyncio==1.6.0\n",
        "networkx==3.4.2\n",
        "nibabel==5.3.2\n",
        "nltk==3.9.1\n",
        "notebook==6.5.7\n",
        "notebook_shim==0.2.4\n",
        "numba==0.60.0\n",
        "numba-cuda==0.2.0\n",
        "numexpr==2.10.2\n",
        "numpy==2.0.2\n",
        "nvidia-cublas-cu12==12.5.3.2\n",
        "nvidia-cuda-cupti-cu12==12.5.82\n",
        "nvidia-cuda-nvcc-cu12==12.5.82\n",
        "nvidia-cuda-nvrtc-cu12==12.5.82\n",
        "nvidia-cuda-runtime-cu12==12.5.82\n",
        "nvidia-cudnn-cu12==9.3.0.75\n",
        "nvidia-cufft-cu12==11.2.3.61\n",
        "nvidia-curand-cu12==10.3.6.82\n",
        "nvidia-cusolver-cu12==11.6.3.83\n",
        "nvidia-cusparse-cu12==12.5.1.3\n",
        "nvidia-cusparselt-cu12==0.6.2\n",
        "nvidia-ml-py==12.570.86\n",
        "nvidia-nccl-cu12==2.21.5\n",
        "nvidia-nvcomp-cu12==4.2.0.11\n",
        "nvidia-nvjitlink-cu12==12.5.82\n",
        "nvidia-nvtx-cu12==12.4.127\n",
        "nvtx==0.2.11\n",
        "nx-cugraph-cu12 @ https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-25.2.0-py3-none-any.whl\n",
        "oauth2client==4.1.3\n",
        "oauthlib==3.2.2\n",
        "openai==1.75.0\n",
        "opencv-contrib-python==4.11.0.86\n",
        "opencv-python==4.11.0.86\n",
        "opencv-python-headless==4.11.0.86\n",
        "openpyxl==3.1.5\n",
        "opentelemetry-api==1.32.1\n",
        "opentelemetry-sdk==1.32.1\n",
        "opentelemetry-semantic-conventions==0.53b1\n",
        "opt_einsum==3.4.0\n",
        "optax==0.2.4\n",
        "optree==0.15.0\n",
        "orbax-checkpoint==0.11.12\n",
        "orjson==3.10.16\n",
        "osqp==1.0.3\n",
        "packaging==24.2\n",
        "pandas==2.2.2\n",
        "pandas-datareader==0.10.0\n",
        "pandas-gbq==0.28.0\n",
        "pandas-stubs==2.2.2.240909\n",
        "pandocfilters==1.5.1\n",
        "panel==1.6.2\n",
        "param==2.2.0\n",
        "parso==0.8.4\n",
        "parsy==2.1\n",
        "partd==1.4.2\n",
        "pathlib==1.0.1\n",
        "patsy==1.0.1\n",
        "peewee==3.17.9\n",
        "peft==0.14.0\n",
        "pexpect==4.9.0\n",
        "pickleshare==0.7.5\n",
        "pillow==11.1.0\n",
        "platformdirs==4.3.7\n",
        "plotly==5.24.1\n",
        "plotnine==0.14.5\n",
        "pluggy==1.5.0\n",
        "ply==3.11\n",
        "polars==1.21.0\n",
        "pooch==1.8.2\n",
        "portpicker==1.5.2\n",
        "preshed==3.0.9\n",
        "prettytable==3.16.0\n",
        "proglog==0.1.11\n",
        "progressbar2==4.5.0\n",
        "prometheus_client==0.21.1\n",
        "promise==2.3\n",
        "prompt_toolkit==3.0.51\n",
        "propcache==0.3.1\n",
        "prophet==1.1.6\n",
        "proto-plus==1.26.1\n",
        "protobuf==5.29.4\n",
        "psutil==5.9.5\n",
        "psycopg2==2.9.10\n",
        "ptyprocess==0.7.0\n",
        "py-cpuinfo==9.0.0\n",
        "py4j==0.10.9.7\n",
        "pyarrow==18.1.0\n",
        "pyasn1==0.6.1\n",
        "pyasn1_modules==0.4.2\n",
        "pycairo==1.28.0\n",
        "pycocotools==2.0.8\n",
        "pycparser==2.22\n",
        "pydantic==2.11.3\n",
        "pydantic_core==2.33.1\n",
        "pydata-google-auth==1.9.1\n",
        "pydot==3.0.4\n",
        "pydotplus==2.0.2\n",
        "PyDrive==1.3.1\n",
        "PyDrive2==1.21.3\n",
        "pyerfa==2.0.1.5\n",
        "pygame==2.6.1\n",
        "pygit2==1.17.0\n",
        "Pygments==2.18.0\n",
        "PyGObject==3.42.0\n",
        "PyJWT==2.10.1\n",
        "pylibcudf-cu12 @ https://pypi.nvidia.com/pylibcudf-cu12/pylibcudf_cu12-25.2.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl\n",
        "pylibcugraph-cu12==25.2.0\n",
        "pylibraft-cu12==25.2.0\n",
        "pymc==5.21.2\n",
        "pymystem3==0.2.0\n",
        "pynndescent==0.5.13\n",
        "pynvjitlink-cu12==0.5.2\n",
        "pynvml==12.0.0\n",
        "pyogrio==0.10.0\n",
        "Pyomo==6.8.2\n",
        "PyOpenGL==3.1.9\n",
        "pyOpenSSL==24.2.1\n",
        "pyparsing==3.2.3\n",
        "pyperclip==1.9.0\n",
        "pyproj==3.7.1\n",
        "pyshp==2.3.1\n",
        "PySocks==1.7.1\n",
        "pyspark==3.5.5\n",
        "pytensor==2.30.3\n",
        "pytest==8.3.5\n",
        "python-apt==0.0.0\n",
        "python-box==7.3.2\n",
        "python-dateutil==2.8.2\n",
        "python-louvain==0.16\n",
        "python-slugify==8.0.4\n",
        "python-snappy==0.7.3\n",
        "python-utils==3.9.1\n",
        "pytz==2025.2\n",
        "pyviz_comms==3.0.4\n",
        "PyYAML==6.0.2\n",
        "pyzmq==24.0.1\n",
        "raft-dask-cu12==25.2.0\n",
        "rapids-dask-dependency==25.2.0\n",
        "ratelim==0.1.6\n",
        "referencing==0.36.2\n",
        "regex==2024.11.6\n",
        "requests==2.32.3\n",
        "requests-oauthlib==2.0.0\n",
        "requests-toolbelt==1.0.0\n",
        "requirements-parser==0.9.0\n",
        "rich==13.9.4\n",
        "rmm-cu12==25.2.0\n",
        "roman-numerals-py==3.1.0\n",
        "rpds-py==0.24.0\n",
        "rpy2==3.5.17\n",
        "rsa==4.9.1\n",
        "safetensors==0.5.3\n",
        "scikit-image==0.25.2\n",
        "scikit-learn==1.6.1\n",
        "scipy==1.14.1\n",
        "scooby==0.10.0\n",
        "scs==3.2.7.post2\n",
        "seaborn==0.13.2\n",
        "SecretStorage==3.3.3\n",
        "Send2Trash==1.8.3\n",
        "sentence-transformers==3.4.1\n",
        "sentencepiece==0.2.0\n",
        "sentry-sdk==2.26.1\n",
        "setproctitle==1.3.5\n",
        "shap==0.47.1\n",
        "shapely==2.1.0\n",
        "shellingham==1.5.4\n",
        "simple-parsing==0.1.7\n",
        "simplejson==3.20.1\n",
        "simsimd==6.2.1\n",
        "six==1.17.0\n",
        "sklearn-compat==0.1.3\n",
        "sklearn-pandas==2.2.0\n",
        "slicer==0.0.8\n",
        "smart-open==7.1.0\n",
        "smmap==5.0.2\n",
        "sniffio==1.3.1\n",
        "snowballstemmer==2.2.0\n",
        "sortedcontainers==2.4.0\n",
        "soundfile==0.13.1\n",
        "soupsieve==2.6\n",
        "soxr==0.5.0.post1\n",
        "spacy==3.8.5\n",
        "spacy-legacy==3.0.12\n",
        "spacy-loggers==1.0.5\n",
        "spanner-graph-notebook==1.1.6\n",
        "Sphinx==8.2.3\n",
        "sphinxcontrib-applehelp==2.0.0\n",
        "sphinxcontrib-devhelp==2.0.0\n",
        "sphinxcontrib-htmlhelp==2.1.0\n",
        "sphinxcontrib-jsmath==1.0.1\n",
        "sphinxcontrib-qthelp==2.0.0\n",
        "sphinxcontrib-serializinghtml==2.0.0\n",
        "SQLAlchemy==2.0.40\n",
        "sqlglot==25.20.2\n",
        "sqlparse==0.5.3\n",
        "srsly==2.5.1\n",
        "stanio==0.5.1\n",
        "statsmodels==0.14.4\n",
        "stringzilla==3.12.4\n",
        "sympy==1.13.1\n",
        "tables==3.10.2\n",
        "tabulate==0.9.0\n",
        "tbb==2022.1.0\n",
        "tblib==3.1.0\n",
        "tcmlib==1.3.0\n",
        "tenacity==9.1.2\n",
        "tensorboard==2.18.0\n",
        "tensorboard-data-server==0.7.2\n",
        "tensorflow==2.18.0\n",
        "tensorflow-datasets==4.9.8\n",
        "tensorflow-hub==0.16.1\n",
        "tensorflow-io-gcs-filesystem==0.37.1\n",
        "tensorflow-metadata==1.17.1\n",
        "tensorflow-probability==0.25.0\n",
        "tensorflow-text==2.18.1\n",
        "tensorflow_decision_forests==1.11.0\n",
        "tensorstore==0.1.73\n",
        "termcolor==3.0.1\n",
        "terminado==0.18.1\n",
        "text-unidecode==1.3\n",
        "textblob==0.19.0\n",
        "tf-slim==1.1.0\n",
        "tf_keras==2.18.0\n",
        "thinc==8.3.6\n",
        "threadpoolctl==3.6.0\n",
        "tifffile==2025.3.30\n",
        "timm==1.0.15\n",
        "tinycss2==1.4.0\n",
        "tokenizers==0.21.1\n",
        "toml==0.10.2\n",
        "toolz==0.12.1\n",
        "torch @ https://download.pytorch.org/whl/cu124/torch-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl\n",
        "torchaudio @ https://download.pytorch.org/whl/cu124/torchaudio-2.6.0%2Bcu124-cp311-cp311-linux_x86_64.whl\n",
        "torchsummary==1.5.1\n",
        "torchvision @ https://download.pytorch.org/whl/cu124/torchvision-0.21.0%2Bcu124-cp311-cp311-linux_x86_64.whl\n",
        "tornado==6.4.2\n",
        "tqdm==4.67.1\n",
        "traitlets==5.7.1\n",
        "traittypes==0.2.1\n",
        "transformers==4.51.3\n",
        "treelite==4.4.1\n",
        "treescope==0.1.9\n",
        "triton==3.2.0\n",
        "tweepy==4.15.0\n",
        "typeguard==4.4.2\n",
        "typer==0.15.2\n",
        "types-pytz==2025.2.0.20250326\n",
        "types-setuptools==79.0.0.20250422\n",
        "typing-inspection==0.4.0\n",
        "typing_extensions==4.13.2\n",
        "tzdata==2025.2\n",
        "tzlocal==5.3.1\n",
        "uc-micro-py==1.0.3\n",
        "ucx-py-cu12==0.42.0\n",
        "ucxx-cu12==0.42.0\n",
        "umap-learn==0.5.7\n",
        "umf==0.10.0\n",
        "uritemplate==4.1.1\n",
        "urllib3==2.3.0\n",
        "vega-datasets==0.9.0\n",
        "visualkeras==0.1.4\n",
        "wadllib==1.3.6\n",
        "wandb==0.19.9\n",
        "wasabi==1.1.3\n",
        "wcwidth==0.2.13\n",
        "weasel==0.4.1\n",
        "webcolors==24.11.1\n",
        "webencodings==0.5.1\n",
        "websocket-client==1.8.0\n",
        "websockets==15.0.1\n",
        "Werkzeug==3.1.3\n",
        "widgetsnbextension==3.6.10\n",
        "wordcloud==1.9.4\n",
        "wrapt==1.17.2\n",
        "wurlitzer==3.1.1\n",
        "xarray==2025.1.2\n",
        "xarray-einstats==0.8.0\n",
        "xgboost==2.1.4\n",
        "xlrd==2.0.1\n",
        "xyzservices==2025.1.0\n",
        "yarl==1.19.0\n",
        "ydf==0.11.0\n",
        "yellowbrick==1.5\n",
        "yfinance==0.2.55\n",
        "zict==3.0.0\n",
        "zipp==3.21.0\n",
        "zstandard==0.23.0\n",
        "```"
      ],
      "metadata": {
        "id": "r5T7DrKw_NYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models. Hooray!"
      ],
      "metadata": {
        "id": "zjWrQnOw9j8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline - Standard CNNs"
      ],
      "metadata": {
        "id": "IvY8Lldm9rht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's Bring in the CIFAR10 Dataset - Quick and Reckless Import"
      ],
      "metadata": {
        "id": "4HOWbC1Bh2Xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Let's try with the CIFAR10 Dataset. Why not?\n",
        "#+ I'm not sure how dataset imports work, so\n",
        "#+ I'm only importing `cifar10` and not\n",
        "#+ from tensorflow.keras import datasets\n",
        "from tensorflow.keras.datasets import cifar10"
      ],
      "metadata": {
        "id": "g3GVr8uzsd_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the data."
      ],
      "metadata": {
        "id": "FDVH0di_tSgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "metadata": {
        "id": "mI4NCr9rsxqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now to start to inspection and the CNN code!"
      ],
      "metadata": {
        "id": "XU0X3sjTw9tS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pick an image"
      ],
      "metadata": {
        "id": "l6Q1_6ixsrAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_for_inspection = x_train[0]"
      ],
      "metadata": {
        "id": "nzPSD-eXstkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at it."
      ],
      "metadata": {
        "id": "pUxh2s9Bs0Vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "plt.grid(False)\n",
        "plt.imshow(image_for_inspection)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UuMpSPt4swlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the image a bit more, so we know how we need to normalize, etc."
      ],
      "metadata": {
        "id": "nICVrJTAgXLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_pixels_to_inspect = 5\n",
        "\n",
        "print(f\"Image shape: {image_for_inspection.shape}\")\n",
        "top_corner_array = \\\n",
        "    image_for_inspection[0:n_pixels_to_inspect, 0:n_pixels_to_inspect, :]\n",
        "print(f\"Top left corner values:\\n{top_corner_array}\")\n"
      ],
      "metadata": {
        "id": "FEJIb6BNgYId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print( (\n",
        "    \"\\n\\nNotice that stacking them side-by-side would give us \\n\"\n",
        "    \"the RGB values as they'd be seen in the image, but \\n\"\n",
        "    \"I'm going for quick stuff. Assuming 255 would be okay, \\n\"\n",
        "    \"or perhaps I should say reasonable, but let's do a better\\n\"\n",
        "    \"check.\\n\"\n",
        "    )\n",
        ")\n",
        "\n",
        "print( (\n",
        "    f\"Minimum single RGB value: {min(image_for_inspection.flatten())}\\n\"\n",
        "    f\"Maximum single RGB value: {max(image_for_inspection.flatten())}\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "H5vMHDelu9Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize pixel values between 0 and 1\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "metadata": {
        "id": "OTKk57-aYt23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Cool visualization of some of our new data, but first\n",
        "#+ let's double check the image size\n",
        "\n",
        "print(f\"Another image's shape: {x_train[1].shape}\")\n",
        "\n",
        "n_pixels_per_side = 32\n",
        "n_channels = 3\n",
        "#  Making sure it's what we expect\n",
        "assert x_train[1].shape == (n_pixels_per_side,\n",
        "                            n_pixels_per_side,\n",
        "                            n_channels\n",
        "       )\n",
        "\n",
        "print(\"If you've gotten here, the shape is correct, \", end=\"\")\n",
        "print(f\"({n_pixels_per_side}, {n_pixels_per_side}, {n_channels})\")"
      ],
      "metadata": {
        "id": "EnCTTYM2Y3Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  The class names with indexes as defined by CIFAR\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "n_classes = len(class_names)\n",
        "\n",
        "#  Let's show some images\n",
        "good_figsize_val = 8\n",
        "plt.figure(figsize=(good_figsize_val, good_figsize_val))\n",
        "\n",
        "n_rows = 5\n",
        "n_cols = n_rows  # Let's make our output of images square\n",
        "\n",
        "n_images_to_show = n_rows * n_cols\n",
        "for i in range(n_images_to_show):\n",
        "  plt.subplot(n_rows, n_cols, i+1)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  plt.grid(False)\n",
        "  plt.imshow(x_train[i])\n",
        "  #  CIFAR labels are arrays, which is\n",
        "  #+ why we need the extra index.\n",
        "  plt.xlabel(class_names[y_train[i][0]])\n",
        "##endof:  for i in range(n_images_to_show)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wAX1pDeGbrLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I want to see exactly what the `filters` parameter is. This can be minimized"
      ],
      "metadata": {
        "id": "dq1GYz80DiFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n'.join(inspect.getdoc(layers.Conv2D).splitlines()[:54]))"
      ],
      "metadata": {
        "id": "vG_cSxxPesht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### I wanted to see exactly what the `filters` parameter was:\n",
        "\n",
        "```\n",
        "filters: int, the dimension of the output space (the number of filters\n",
        "         in the convolution).\n",
        "```"
      ],
      "metadata": {
        "id": "pc5pqXrVmE0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Construction"
      ],
      "metadata": {
        "id": "Xr-R1_-MtJHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Shell/Skeleton"
      ],
      "metadata": {
        "id": "6BbHz60utyGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_0 = models.Sequential()  # the shell of our model"
      ],
      "metadata": {
        "id": "fkMbmH5BtDpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Input Layer"
      ],
      "metadata": {
        "id": "vFXbJJihtSIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Input layer\n",
        "\n",
        "n_pix = n_pixels_per_side\n",
        "\n",
        "in_0 = layers.Input(shape=(n_pix, n_pix, n_channels))\n",
        "model_0.add(in_0)"
      ],
      "metadata": {
        "id": "FYQRlH3-V6sM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.summary()"
      ],
      "metadata": {
        "id": "igDdlBIBtje2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First Convolutional Layer"
      ],
      "metadata": {
        "id": "L0X2OTjAtvFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  First convolutional layer\n",
        "\n",
        "n_filters_1 = 32\n",
        "\n",
        "conv_1 = layers.Conv2D(filters=n_filters_1,\n",
        "                       kernel_size=(3, 3),\n",
        "                       padding='same'\n",
        "                )\n",
        "\n",
        "model_0.add(conv_1)\n",
        "model_0.add(layers.Activation('relu'))\n",
        "\n",
        "##  Same as\n",
        "#conv_1_2 = layers.Conv2D(filters=32,\n",
        "#                         kernel_size=(3, 3),\n",
        "#                         activation='relu',\n",
        "#                         padding='same'\n",
        "#           )"
      ],
      "metadata": {
        "id": "6oQCxrAOtbDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.summary()"
      ],
      "metadata": {
        "id": "9lvRx40zuEJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### First Pooling Layer"
      ],
      "metadata": {
        "id": "ezJ1ef8It9wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  First pooling layer\n",
        "\n",
        "pool_1 = layers.MaxPooling2D(pool_size=(2, 2),\n",
        "                             strides=2,\n",
        "                             padding='valid'\n",
        "                )\n",
        "  #  Note that `strides=2` is the default for `pool_size=(2,2)`,\n",
        "  #+ but I want to make sure I see details. `padding='valid'`\n",
        "  #+ is also default.\n",
        "\n",
        "model_0.add(pool_1)"
      ],
      "metadata": {
        "id": "nVE_bFeNthNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.summary()"
      ],
      "metadata": {
        "id": "KPMjdOzAof4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second Convolutional Layer and Pooling"
      ],
      "metadata": {
        "id": "od0mA_G0uTKS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_filters_2 = 64\n",
        "\n",
        "conv_2 = layers.Conv2D(filters=n_filters_2,\n",
        "                       kernel_size=(3, 3),\n",
        "                       padding='same'\n",
        "                )\n",
        "\n",
        "model_0.add(conv_2)\n",
        "model_0.add(layers.Activation('relu'))\n",
        "\n",
        "pool_2 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "model_0.add(pool_2)"
      ],
      "metadata": {
        "id": "ZxQvL1u8uaaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.summary()"
      ],
      "metadata": {
        "id": "GawqxkGluac4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Third, and for this one, Last Convolutional Layer and Pooling"
      ],
      "metadata": {
        "id": "X28ucwbseXE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_filters_3 = 64\n",
        "\n",
        "conv_3 = layers.Conv2D(filters=n_filters_3,\n",
        "                       kernel_size=(3, 3),\n",
        "                       padding='same'\n",
        "                )\n",
        "\n",
        "model_0.add(conv_3)\n",
        "model_0.add(layers.Activation('relu'))\n",
        "\n",
        "pool_3 = layers.MaxPooling2D(pool_size=(2, 2))\n",
        "\n",
        "model_0.add(pool_3)"
      ],
      "metadata": {
        "id": "7e189Mnqedor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.summary()"
      ],
      "metadata": {
        "id": "rXuIlv9iuae0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flattening and the Fully Connected Layer"
      ],
      "metadata": {
        "id": "0BRTZ0MNeuvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Fully Connected layer (after flattening)\n",
        "\n",
        "model_0.add(layers.Flatten())\n",
        "\n",
        "fc_layer = layers.Dense(units=64)\n",
        "model_0.add(fc_layer)\n",
        "model_0.add(layers.Activation('relu'))"
      ],
      "metadata": {
        "id": "_qhWZDMKs1Pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.summary()"
      ],
      "metadata": {
        "id": "p1mmw0MXe5WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Classification Head"
      ],
      "metadata": {
        "id": "yEKrgOSTe8Uy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decision_head = layers.Dense(units=n_classes)\n",
        "model_0.add(decision_head)\n",
        "##no activation for output (not one-hot encoded)#\n",
        "##model_0.add(layers.Activation('softmax'))"
      ],
      "metadata": {
        "id": "cf404qZje_YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.summary()"
      ],
      "metadata": {
        "id": "KZ5Kde1Sf7WB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model_0)"
      ],
      "metadata": {
        "id": "6ovwP-F0XRKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `print` didn't give us a super-nice summary or visualization, and the summarization is incomplete, so let's look at a few options for visualization"
      ],
      "metadata": {
        "id": "D9N015ol0Ic-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "visualkeras.layered_view(model_0)"
      ],
      "metadata": {
        "id": "AQmXS2EN1VS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "visualkeras.layered_view(model_0)"
      ],
      "metadata": {
        "id": "Am1TM2Ol1aAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  We can better that\n",
        "\n",
        "!stat /usr/local/share/fonts\n",
        "!echo\n",
        "!ls -laH /usr/local/share/fonts/\n",
        "!echo\n",
        "!stat /usr/share/fonts/.uuid/\n",
        "\n",
        "#  Well, maybe not with a quick-and-reckless run.\n",
        "#+ We'd have to download/build/whatever the fonts.\n",
        "#+ Something like\n",
        "#+ ```bash from jupyter\n",
        "#+ !sudo apt-get install ttf-mscorefonts-installer`\n",
        "#+ !sudo fc-cache -f\n",
        "#+ # and check it\n",
        "#+ !fc-match Arial\n",
        "#+ ```\n",
        "\n",
        "#  But I don't know how Jupyter's bash works exactly,\n",
        "#+ nor do I remember the details of `fc-cache`. It\n",
        "#+ might require `source ~/.bashrc` before workout.\n",
        "#+ For all that, the TIME IS NOT NOW"
      ],
      "metadata": {
        "id": "eAZuvF441kT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  What would have come was ...\n",
        "#font = ImageFont.truetype(\"arial.ttf\", 32)\n",
        "visualkeras.layered_view(model_0, legend=True) #, font=font)\n",
        "\n",
        "# Easy fix, just leave out the font parameter"
      ],
      "metadata": {
        "id": "7Ogoain620sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compile Using Optimizer and Loss, Specify Metrics to be Reported"
      ],
      "metadata": {
        "id": "FUTTOC1Ig_kA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compile_tic = timeit.default_timer()\n",
        "model_0.compile(\n",
        "    optimizer=optimizers.Adam(), #default learning rate?\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "compile_toc = timeit.default_timer()\n",
        "print(f\"Compile time: {compile_toc - compile_tic:0.4f} seconds\")"
      ],
      "metadata": {
        "id": "Ek9bgsOrf92R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train of it, with both viewing and logging"
      ],
      "metadata": {
        "id": "PxBScfbi5B69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "cf. [tensorboard documentation and example](https://www.tensorflow.org/tensorboard/graphs), which doesn't seem to want to play with the Wayback Machine (archive.org)"
      ],
      "metadata": {
        "id": "HjCJPwgCzJ_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "C_UVHFQs1FBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!stat ./tblogs/\n",
        "# if necessary,\n",
        "#!rm -rf ./tblogs/"
      ],
      "metadata": {
        "id": "MoCKl0zXz2_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  if necessary, (output is anything but\n",
        "#+                `stat: cannot statx './tblogs/':`\n",
        "#+                ` No such file or directory`),\n",
        "#+\n",
        "#+ Actually, I just have it telling you it can't if it can't\n",
        "!rm -rf ./tblogs/ && echo \"No problem came up with removal\" || \\\n",
        "echo -e \"Remvoing ./tblogs/ didn't work\\n\"\\\n",
        "\"likely because it didn't exist\\nLet me check ...\"\n",
        "!echo\n",
        "!echo \"Checking stat, whether there's a problem or not\"\n",
        "!stat ./tblogs/ || echo \"Not able to stat, dir likely nonexistent\""
      ],
      "metadata": {
        "id": "j_G9Xfs-0d0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logdir = \"tblogs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = callbacks.TensorBoard(log_dir=logdir)"
      ],
      "metadata": {
        "id": "EXnP5SKHzSLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cf. [StackOverflow Discussion](https://web.archive.org/web/20250423184118/https://stackoverflow.com/questions/67757496/tensorflow-keras-print-out-and-save-loss-and-gradients-during-model-fit)"
      ],
      "metadata": {
        "id": "wW2dPZyj9B4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainableVariablesCallback(callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    print(model_0.trainable_variables)\n",
        "  ##endof:  on_epoch_end\n",
        "##endof:  TrainableVariablesCallback"
      ],
      "metadata": {
        "id": "ZmZUtTPc9BeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "@todo : Get compute use, memory use, etc. and log, e.g., the trainable variables"
      ],
      "metadata": {
        "id": "pl9lMAfk_X1q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<strong>REMEMBER TO CHANGE YOUR RUNTIME TO GPU</strong>\n",
        "\n",
        "(You might need to re-run old cells, but it's worth it.)"
      ],
      "metadata": {
        "id": "5Ei-1iPxA2wQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Num GPUs Available: {len(tf.config.list_physical_devices('GPU'))}\")"
      ],
      "metadata": {
        "id": "k8IqCOm4GlUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  This one gives way too much output. If desired, I can\n",
        "##+ figure out how to send it to a file.\n",
        "# my_trainable_vars_logger = TrainableVariablesCallback()\n",
        "\n",
        "!>training.log  # empty the file without deleting the name\n",
        "my_csv_logger = callbacks.CSVLogger('training.log')\n",
        "\n",
        "n_epochs = 10\n",
        "\n",
        "train_tic = timeit.default_timer()\n",
        "\n",
        "history = model_0.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=n_epochs,\n",
        "    validation_data=(x_test, y_test),\n",
        "    callbacks=[my_csv_logger,\n",
        "               #, my_trainable_vars_logger\n",
        "              tensorboard_callback\n",
        "    ]\n",
        ")\n",
        "\n",
        "train_toc = timeit.default_timer()\n",
        "print(f\"Train time: {train_toc - train_tic:0.4f} seconds\")\n",
        "\n",
        "train_time_str = format_timespan(train_toc - train_tic)\n",
        "print(f\"which equates to: {train_time_str}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "vmHzFHcJ5DZ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir tblogs  # This is having problems. It seems to give nice loss/accuracy curves that show we are overfitting by a lot."
      ],
      "metadata": {
        "collapsed": true,
        "id": "PW53ta1P1W_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## You can use the process number it gives you to kill the server\n",
        "# !kill 11839"
      ],
      "metadata": {
        "id": "g6TQUpwt5axL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standard View of the Accuracy and Loss During Training, maybe more."
      ],
      "metadata": {
        "id": "bG1lcn3sFd8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!head -n 20 training.log\n",
        "!tail -n 20 training.log"
      ],
      "metadata": {
        "id": "rruXIxv1geJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Okay, there wasn't enough stuff to justify the\n",
        "#+ `head` and `tail`, but it's nice to check things\n",
        "#+ out\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.legend(loc='lower right')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KUtdeL61Fu1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim([0, 1.2])\n",
        "plt.legend(loc='lower right')"
      ],
      "metadata": {
        "id": "hous4TID54i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That looks like BAD overfitting to me. Maybe it's because we only used 10 epochs. The accuracy is better than chance. Hmmm. I'm going to rerun it\n",
        "with 10 epochs and the Grad-CAM, then with 100 epochs and the Grad-Cam"
      ],
      "metadata": {
        "id": "tnJOzgD56SKN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## And now, run it on our test set."
      ],
      "metadata": {
        "id": "SIpt0auDF2Hv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_locc, test_acc = model_0.evaluate(\n",
        "    x_test,  y_test,\n",
        "    verbose=2)"
      ],
      "metadata": {
        "id": "HYtoM2RGFzfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gFy85nkc7Lml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Here are some nice pictures that will be goals for visualizations <br/>and explanations of Grad-CAM that should help explain things."
      ],
      "metadata": {
        "id": "xKGhr2EDo15Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lots of these come from a nice site, learnopencv.com\n",
        "\n",
        "I feel the creators do a really good job with visualization. The other sources are below all the images."
      ],
      "metadata": {
        "id": "D2vpRMnpoLNT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br/>\n",
        "<div>\n",
        "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/cnn_analyticsvidhya.png\"\n",
        "       alt=\"CNN\"\n",
        "       width=\"750px\">\n",
        "</div>\n",
        "<br/>"
      ],
      "metadata": {
        "id": "zrWe8esjxxmR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br/>\n",
        "<div>\n",
        "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/cnn_learnopencv_04.png\"\n",
        "       alt=\"CNN\"\n",
        "       width=\"750px\">\n",
        "</div>\n",
        "<br/>"
      ],
      "metadata": {
        "id": "1Qg1HxAoxxkI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br/>\n",
        "<div>\n",
        "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/cnn_learnopencv_05.png\"\n",
        "       alt=\"CNN\"\n",
        "       width=\"750px\">\n",
        "</div>\n",
        "<br/>"
      ],
      "metadata": {
        "id": "Hzo4vcVBxxh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br/>\n",
        "<div>\n",
        "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/cnn_learnopencv_06.png\"\n",
        "       alt=\"CNN\"\n",
        "       width=\"1000px\">\n",
        "</div>\n",
        "<br/>"
      ],
      "metadata": {
        "id": "rT3pClxDxxfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br/>\n",
        "<div>\n",
        "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/cnn_learnopencv_01.png\"\n",
        "       alt=\"CNN Code and Architecture Mix\"\n",
        "       width=\"500px\">\n",
        "</div>\n",
        "<br/>"
      ],
      "metadata": {
        "id": "7ZOnCGF2xxbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That shows a bit how we get to a point where they might trust us. Now observe the Grad-CAM wonder."
      ],
      "metadata": {
        "id": "VL4vpFFHng4i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br/>\n",
        "<div>\n",
        "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/orig_image_grad_camm_fig_2-1.png\"\n",
        "       alt=\"Grad-CAM Cat and Dog, Original\"\n",
        "       width=\"500px\">\n",
        "</div>\n",
        "<br/>"
      ],
      "metadata": {
        "id": "Kd1rmlf3xxZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br/>\n",
        "<div>\n",
        "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/combination_dog_guidedbackprop_and_gradcam_then_ResNet.png\"\n",
        "       alt=\"Grad-Cam for dog, combined\"\n",
        "       width=\"500px\">\n",
        "</div>\n",
        "<br/>"
      ],
      "metadata": {
        "id": "oJjC5J_fptgo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br/>\n",
        "<div>\n",
        "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/combination_dog_guidedbackprop_and_gradcam_then_ResNet.png\"\n",
        "       alt=\"Grad-Cam Dog details, combined\"\n",
        "       width=\"500px\">\n",
        "</div>\n",
        "<br/>"
      ],
      "metadata": {
        "id": "CtbnkVL8xxVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty convincing for me. I would definitely call this explainable AI."
      ],
      "metadata": {
        "id": "Eclt_KMSxxQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br/>\n",
        "<div>\n",
        "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/interesting_semantic_segmentation_gradcam.png\"\n",
        "       alt=\"Grad-Cam\"\n",
        "       width=\"500px\">\n",
        "</div>\n",
        "<br/>"
      ],
      "metadata": {
        "id": "zaOr87WZxxLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<br/>\n",
        "<div>\n",
        "  <img src=\"https://raw.githubusercontent.com/bballdave025/rib-wrist-in-bin-din/refs/heads/main/img/decisions_for_right_reasons_gender_issue_gradcam.png\"\n",
        "       alt=\"Grad-Cam important gender issue\"\n",
        "       width=\"500px\">\n",
        "</div>\n",
        "<br/>"
      ],
      "metadata": {
        "id": "1XpuMOGAxxFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pyimagesearch.com/2021/07/19/pytorch-training-your-first-convolutional-neural-network-cnn/"
      ],
      "metadata": {
        "id": "RCtmkpi8gd54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-yGHUa5PL4A"
      ],
      "metadata": {
        "id": "p2NDxK9Ege9B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-Tuning ResNet50 A Practical Guide by It's Amit Jan 2025"
      ],
      "metadata": {
        "id": "Y-BaDy1rcdgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://adamharley.com/nn_vis/cnn/3d.html"
      ],
      "metadata": {
        "id": "OnIgx-71ptCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/ashishpatel26/Tools-to-Design-or-Visualize-Architecture-of-Neural-Network\n",
        "https://datascience.stackexchange.com/questions/12851/how-do-you-visualize-neural-network-architectures\n",
        "https://datascience.stackexchange.com/questions/2670/visualizing-deep-neural-network-training\n",
        "https://learnopencv.com/understanding-convolutional-neural-networks-cnn/\n",
        "https://www.analyticsvidhya.com/blog/2020/10/what-is-the-convolutional-neural-network-architecture/\n"
      ],
      "metadata": {
        "id": "ylzaU8CQps0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.researchgate.net/figure/The-overall-LeNet-architecture-The-numbers-at-the-convolution-and-pooling-layers_fig2_318972455"
      ],
      "metadata": {
        "id": "wbaKnpc3psWC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pyimagesearch.com/2020/03/09/grad-cam-visualize-class-activation-maps-with-keras-tensorflow-and-deep-learning/\n",
        "\n",
        "https://github.com/jacobgil/pytorch-grad-cam"
      ],
      "metadata": {
        "id": "kCa4u7DTv4bQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kYq0bZXxgd2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Super ResNet 50 (including attention, extra dropout, and other extras)\n"
      ],
      "metadata": {
        "id": "EzCD1fBPM1rJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code basics for putting on AWS next time. Let's run model visualizations at\n",
        "each step."
      ],
      "metadata": {
        "id": "7G08gQ2vM-W6"
      }
    }
  ]
}